:PROPERTIES:
:ID:       fb6e5467-0a99-4208-8127-fc41502a38d3
:END:
#+title: AI's bias problem
#+cite_export: csl /home/kjani/Zotero/styles/oscola.csl

It is my hypothesis that AI's bias problem is closely linked to [[id:6094e3fa-2e35-4c1e-b695-52823cb51782][AI's blackbox problem]]. The presence of a non-auditable core makes it infinitely more difficult to probe and fix the underlying cause behind the bias or toxicity that has been repeatedly flagged. The use of AI in E-Proctoring has been flagged as discriminatory for disabled persons. It argues that it could hold persons who are twitching, moving around or having spasms  as having cheated because it does not recognise the infinitely complex nature of human behaviour especially around specially-abled persons.

This is a fair assesment, but it fails to take into consideration that such events can also take place with persons who are not differentlty abled and were bona-fide giving their paper. The problem is not that it discriminates against those who are disabled. This analysis hides a  far more complex issue which is as follows: That the AI will terminate one's exam without clearly explaining the factors behind such termination. This again points back to our black-box problem which states that if one wishes to continue to leverage AI in high-stakes systems they must deal its irrationality. Over this, I have previousy pointed out that the source of this problem is the lack of open-ness about traning weights, data-sources and most importantly the function of the transformer. It is simpler to take into appeal one-off cases where the AI may end up discriminating against differently-abled persons if we have a clear account of how and on what ground was the termination decision taken by the proctoring AI.


